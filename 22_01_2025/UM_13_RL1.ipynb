{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "── \u001b[1mAttaching core tidyverse packages\u001b[22m ──────────────────────── tidyverse 2.0.0 ──\n",
      "\u001b[32m✔\u001b[39m \u001b[34mdplyr    \u001b[39m 1.1.4     \u001b[32m✔\u001b[39m \u001b[34mreadr    \u001b[39m 2.1.5\n",
      "\u001b[32m✔\u001b[39m \u001b[34mforcats  \u001b[39m 1.0.0     \u001b[32m✔\u001b[39m \u001b[34mstringr  \u001b[39m 1.5.1\n",
      "\u001b[32m✔\u001b[39m \u001b[34mggplot2  \u001b[39m 3.5.1     \u001b[32m✔\u001b[39m \u001b[34mtibble   \u001b[39m 3.2.1\n",
      "\u001b[32m✔\u001b[39m \u001b[34mlubridate\u001b[39m 1.9.3     \u001b[32m✔\u001b[39m \u001b[34mtidyr    \u001b[39m 1.3.1\n",
      "\u001b[32m✔\u001b[39m \u001b[34mpurrr    \u001b[39m 1.0.2     \n",
      "── \u001b[1mConflicts\u001b[22m ────────────────────────────────────────── tidyverse_conflicts() ──\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mfilter()\u001b[39m masks \u001b[34mstats\u001b[39m::filter()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mlag()\u001b[39m    masks \u001b[34mstats\u001b[39m::lag()\n",
      "\u001b[36mℹ\u001b[39m Use the conflicted package (\u001b[3m\u001b[34m<http://conflicted.r-lib.org/>\u001b[39m\u001b[23m) to force all conflicts to become errors\n"
     ]
    }
   ],
   "source": [
    "library(tidyverse)\n",
    "options(jupyter.rich_display=FALSE,\n",
    "        repr.plot.width=15,\n",
    "        repr.plot.height=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "source": [
    "# Uczenie maszynowe\n",
    "## 22.01.2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Podstawy uczenia ze wzmocnieniem**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uczenie przez wzmacnianie (ang. RL - *Reinforcement Learning*) jest trzecim z głównych nurtów uczenia\n",
    "maszynowego. Jego zadaniem jest interakcja ze środowiskiem na podstawiez bieranych informacji. W uczeniu\n",
    "ze wzmocnieniem wyróżnia się trzy główne elementy:  \n",
    "- **Środowisko**: zadanie lub symulacja, z którym algorytm wchodzi w interakcje. Celem uczenia jest\n",
    "maksymalizacja nagrody zwracanej przez środowisko, czyli nauczenie agenta osiągania w nim na-\n",
    "jwyższego wyniku (np wygrania największej ilości gier);\n",
    "- **Agent**: wchodzi w interakcję ze środowiskiem, ucząc się najkorzystniejszego oddziaływania z nim. Za\n",
    "zachowanie agenta odpowiada tzw. polityka, czyli funkcja zwracająca akcję (najczęściej jest to sieć\n",
    "neuronowa);\n",
    "- **Bufor**: magazyn danych przechowujący informacje zebrane przez agenta w trakcie uczenia, które\n",
    "następnie są wykorzystywane do jego wytrenowania.  \n",
    "\n",
    "W ogólnym przypadku agent może nie mieć pełnej informacji o swoim środowisku, jak również precyzyjnego\n",
    "(a nwet żadnego) opisu swoich działań i ich skutków. Inaczej mówiąc, agent zostaje umieszczony w\n",
    "środowisku, którego nie zna, i musi się nauczyć skutecznie w nim działać, aby maksymalizować pewne\n",
    "kryterium, udostępniane mu w formie wzmocnień."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Procesy decyzyjne Markowa**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zakłada się , że probabilistyczny model skutków akcji agenta jako zagadnienie podstawowe jest dyskretnym\n",
    "procesem Markowa (MDP), jednak agent nie zna jego parametrów.\n",
    "W procesach decyzyjnych Markowa przejścia między stanami zależą od stanu bieżącego i wektora akcji, który\n",
    "jest stosowany do danego systemu. Formalnie MDP jest określony przez:  \n",
    "• zbiór możliwych stanów S  \n",
    "• zbiór możliwych akcji A  \n",
    "• nagrody/kary R  \n",
    "• polityka π∗  \n",
    "• wartość, v  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zadanie polega na zbadaniu możliwych stanów poprzez podejmowanie działań Ai wymyśleniu optymalnej\n",
    "polityki π∗, która maksymalizuje wartości v na podstawie nagród i kar R."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDPtoolbox\n",
    "Pakiet MDPToolbox wykorzystuje procesy Markowa do nauki wzmacniania.\n",
    "1. Aby zdefiniować elementy uczenia RL, należy przypisać etykiety do każdego ze stanów w macierzy\n",
    "nawigacyjnej. Na początek rozważmy macierz 2x2:  \n",
    "![](macierz2x2.png)  \n",
    "S1 jest stanem początkowym, S4 - końcowym. Nie można przejść bezpośrednio z S1 do S4 z powodu ściany.\n",
    "Zatem z S1 można tylko przejść do S2 lub pozostać w S1.\n",
    "Stąd macierz w dół będzie miała niezerowe prawdopodobieństwa tylko dla S1 i S2 w pierwszym wierszu.\n",
    "Podobnie możemy zdefiniować prawdopodobieństwa dla każdej akcji w każdym stanie.\n",
    "\n",
    "2. Teraz możemy zdefiniować zbiór akcji: up, down, left, right dla macierzy stanów 2x2. Uwaga: jest\n",
    "to macierz prawdopodobieństw, gdzie w każdym wierszu ich suma musi wynosić 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Up Action\n",
    "up <- matrix(c(\n",
    "    1.0, 0.0, 0.0, 0.0,\n",
    "    0.7, 0.2, 0.1, 0.0,\n",
    "    0.0, 0.1, 0.2, 0.7,\n",
    "    0.0, 0.0, 0.0, 1.0),\n",
    "    nrow=4, ncol=4, byrow=TRUE)\n",
    "\n",
    "# Down Action\n",
    "down <- matrix(c(\n",
    "    0.3, 0.7, 0.0, 0.0,\n",
    "    0.0, 0.9, 0.1, 0.0,\n",
    "    0.0, 0.1, 0.9, 0.0,\n",
    "    0.0, 0.0, 0.7, 0.3),\n",
    "    nrow=4, ncol=4, byrow=TRUE)\n",
    "\n",
    "# Left Action\n",
    "left <- matrix(c(\n",
    "    0.9, 0.1, 0.0, 0.0,\n",
    "    0.1, 0.9, 0.0, 0.0,\n",
    "    0.0, 0.7, 0.2, 0.1,\n",
    "    0.0, 0.0, 0.1, 0.9),\n",
    "    nrow=4, ncol=4, byrow=TRUE)\n",
    "\n",
    "# Right Action\n",
    "right <- matrix(c(\n",
    "    0.9, 0.1, 0.0, 0.0,\n",
    "    0.1, 0.2, 0.7, 0.0,\n",
    "    0.0, 0.0, 0.9, 0.1,\n",
    "    0.0, 0.0, 0.1, 0.9), \n",
    "    nrow=4, ncol=4, byrow=TRUE)\n",
    "\n",
    "# Combined Actions matrix\n",
    "actions <- list(up = up, down = down, left = left, right = right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Zdefiniowanie kar i nagród  \n",
    "Jedyną karą jest mała kara za każdy dodatkowy krok. Określmy ją jako -1.\n",
    "Nagroda jest otrzymywana po osiągnięciu stanu S4. Ustawmy wagę na poziomie +10. W ten sposób możemy\n",
    "utworzyć macierz R:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "rewards <- matrix(c(\n",
    "    -1, -1, -1, -1,\n",
    "    -1, -1, -1, -1,\n",
    "    -1, -1, -1, -1,\n",
    "    10, 10, 10, 10),\n",
    "    nrow=4, ncol=4, byrow=TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Następnie algorytm musi znaleźć optymalną politykę i jej wartość. W tym celu posłużymy się funkcją\n",
    "mdp_policy_iteration(), która wymaga ustawienia akcji, nagród oraz rabatu jako danych wejś-\n",
    "ciowych do obliczenia wyników. Rabat jest używany do zmniejszenia wartości bieżącej nagrody lub\n",
    "kary w miarę wykonywania każdego z kroków."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: Matrix\n",
      "\n",
      "\n",
      "Attaching package: ‘Matrix’\n",
      "\n",
      "\n",
      "The following objects are masked from ‘package:tidyr’:\n",
      "\n",
      "    expand, pack, unpack\n",
      "\n",
      "\n",
      "Loading required package: linprog\n",
      "\n",
      "Loading required package: lpSolve\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library(MDPtoolbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "solver <- MDPtoolbox::mdp_policy_iteration(P = actions,  R = rewards, discount = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wynik daje nam politykę, wartość na każdym kroku oraz dodatkowo liczbę iteracji i czas potrzebny na\n",
    "wykonanie. Jak wiemy, polityka powinna określać właściwą ścieżkę do osiągnięcia stanu końcowego S4.\n",
    "Używamy funkcji polityki, aby poznać macierze używane do definiowania polityki, a następnie nazwy z listy\n",
    "działań."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1] 2 4 1 1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "solver$policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1] \"down\"  \"right\" \"up\"    \"up\"   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "names(actions)[solver$policy]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "source": [
    "Wartości są zawarte w v i pokazują nagrodę na każdym kroku."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1] -1.106604 -1.048661 -0.237458 11.111111"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "solver$V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iteracje i czas mogą być użyte do śledzenia liczby iteracji oraz czasu, aby monitorować złożoność."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1] 2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "solver$iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Time difference of 0.01673484 secs"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "solver$time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "source": [
    "5. Spróbuj wykonać powyższe kroki dla macierzy 3x3.  \n",
    " <img src=\"macierz3x3.png\" alt=\"drawing\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Up Action\n",
    "up <- matrix(c(\n",
    "#   s1   s2   s3   s4   s5   s6   s7   s8   s9   \n",
    "    0.9, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,  # s1\n",
    "    0.1, 0.7, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,  # s2\n",
    "    0.0, 0.1, 0.7, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0,  # s3\n",
    "    0.0, 0.0, 0.1, 0.1, 0.8, 0.0, 0.0, 0.0, 0.0,  # s4\n",
    "    0.0, 0.0, 0.0, 0.1, 0.1, 0.8, 0.0, 0.0, 0.0,  # s5\n",
    "    0.0, 0.0, 0.0, 0.0, 0.1, 0.7, 0.2, 0.0, 0.0,  # s6\n",
    "    0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.7, 0.2, 0.0,  # s7\n",
    "    0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.7, 0.2,  # s8\n",
    "    0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.9),# s9\n",
    "    nrow=9, ncol=9, byrow=TRUE)\n",
    "\n",
    "# Down Action\n",
    "down <- matrix(c(\n",
    "#   s1   s2   s3   s4   s5   s6   s7   s8   s9   \n",
    "    0.3, 0.7, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,  # s1\n",
    "    0.1, 0.1, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,  # s2\n",
    "    0.0, 0.1, 0.7, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0,  # s3\n",
    "    0.0, 0.0, 0.1, 0.7, 0.2, 0.0, 0.0, 0.0, 0.0,  # s4\n",
    "    0.0, 0.0, 0.0, 0.1, 0.7, 0.2, 0.0, 0.0, 0.0,  # s5\n",
    "    0.0, 0.0, 0.0, 0.0, 0.1, 0.7, 0.2, 0.0, 0.0,  # s6\n",
    "    0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.1, 0.8, 0.0,  # s7\n",
    "    0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.1, 0.8,  # s8\n",
    "    0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.9),# s9\n",
    "    nrow=9, ncol=9, byrow=TRUE)\n",
    "\n",
    "# Left Action\n",
    "left <- matrix(c(\n",
    "#   s1   s2   s3   s4   s5   s6   s7   s8   s9   \n",
    "    0.9, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,  # s1\n",
    "    0.1, 0.7, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,  # s2\n",
    "    0.0, 0.1, 0.7, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0,  # s3\n",
    "    0.0, 0.0, 0.1, 0.7, 0.2, 0.0, 0.0, 0.0, 0.0,  # s4\n",
    "    0.0, 0.0, 0.0, 0.1, 0.7, 0.2, 0.0, 0.0, 0.0,  # s5\n",
    "    0.0, 0.0, 0.0, 0.0, 0.1, 0.7, 0.2, 0.0, 0.0,  # s6\n",
    "    0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.7, 0.2, 0.0,  # s7\n",
    "    0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.7, 0.2,  # s8\n",
    "    0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.9),# s9\n",
    "    nrow=9, ncol=9, byrow=TRUE)\n",
    "\n",
    "# Right Action\n",
    "right <- matrix(c(\n",
    "#   s1   s2   s3   s4   s5   s6   s7   s8   s9   \n",
    "    0.9, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,  # s1\n",
    "    0.1, 0.7, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,  # s2\n",
    "    0.0, 0.1, 0.1, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0,  # s3\n",
    "    0.0, 0.0, 0.1, 0.7, 0.2, 0.0, 0.0, 0.0, 0.0,  # s4\n",
    "    0.0, 0.0, 0.0, 0.1, 0.7, 0.2, 0.0, 0.0, 0.0,  # s5\n",
    "    0.0, 0.0, 0.0, 0.0, 0.1, 0.1, 0.8, 0.0, 0.0,  # s6\n",
    "    0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.7, 0.2, 0.0,  # s7\n",
    "    0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.7, 0.2,  # s8\n",
    "    0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.9), # s9\n",
    "    nrow=9, ncol=9, byrow=TRUE)\n",
    "\n",
    "# Combined Actions matrix\n",
    "actions <- list(up = up, down = down, left = left, right = right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "rewards <- matrix(c(\n",
    "    -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
    "    -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
    "    -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
    "    -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
    "    -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
    "    -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
    "    -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
    "    -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
    "    10, 10, 10, 10, 10, 10, 10, 10, 10),\n",
    "    nrow=9, ncol=9, byrow=TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "solver=mdp_policy_iteration(P=actions, R=rewards, discount = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1] 5 5 5 5 5 5 5 5 1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "solver$policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1] NA   NA   NA   NA   NA   NA   NA   NA   \"up\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "names(actions)[solver$policy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000\n",
       "[9] 10.98901"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "solver$V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1] 1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "solver$iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Time difference of 0.001503944 secs"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "solver$time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*wyszlo: nic, ale nie umiem tego jeszcze na tyle zeby ogarnac czemu*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
